{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Samples\n",
        "\n",
        "In this notebook we show how to generate samples using a trained model.\n",
        "\n",
        "Before running the notebook make sure to upload the ```dataset.zip``` file, the ```denoising_diffusion_pytorch.py``` file (can be found [here](https://github.com/Lilac-code/music-diffusion/tree/main)), and the ```checkpoint.pth``` file.\n",
        "\n",
        "If using Kaggle, then upload these as datasets named 'dataset', 'unetfile' and 'checkpoint' respectively.\n",
        "\n",
        "In our Github directory our pre-trained model can be found as well (named ```checkpoint.pth```)."
      ],
      "metadata": {
        "id": "nYzSegP__hn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model"
      ],
      "metadata": {
        "id": "tNT8QyTY_uIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly we unzip the ```dataset.zip``` file (we need it to calculate the ratio), and install all the dependencies."
      ],
      "metadata": {
        "id": "RSUzBxGoHKgy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rooehGUrSqmZ"
      },
      "outputs": [],
      "source": [
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ema_pytorch\n",
        "!pip install einops\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "eiG13hPGS3k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from torchvision import transforms as T, utils\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "# the unet implementation from https://github.com/lucidrains/denoising-diffusion-pytorch\n",
        "from denoising_diffusion_pytorch import Unet\n",
        "\n",
        "transform = T.Compose([T.ToTensor()])"
      ],
      "metadata": {
        "id": "VbxQZmV4S-_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segments=[]\n",
        "for img in os.listdir('dataset'):\n",
        "  f = os.path.join('dataset', img)\n",
        "  image=Image.open(f)\n",
        "  image=transform(image)\n",
        "  segments.append(image)"
      ],
      "metadata": {
        "id": "oyDr1DjqTBwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_ratio():\n",
        "  ratio=0\n",
        "  for im in segments:\n",
        "    ratio+=torch.sum(im).item()\n",
        "  ratio/=len(segments)\n",
        "  ratio/=len(segments[0][0])\n",
        "  ratio/=len(segments[0][0][0])\n",
        "  return ratio"
      ],
      "metadata": {
        "id": "GZP0-8J9TEWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function will become handy later, as an easy way to evaluate a generated sample."
      ],
      "metadata": {
        "id": "ehfdT9WTDrwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_sample_ratio(sample):\n",
        "  ratio=0\n",
        "  ratio+=np.sum(sample)\n",
        "  ratio/=sample.size\n",
        "  return ratio"
      ],
      "metadata": {
        "id": "9efKrMzJTh0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = './checkpoint.pth'"
      ],
      "metadata": {
        "id": "kU2EW-Y-Tl31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model (if the model was trained on 2 GPUs then execute the cell after this)"
      ],
      "metadata": {
        "id": "0WEeFTOHUgqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unet = Unet(dim=48, channels=1, resnet_block_groups=3, dim_mults=(1, 2, 4, 4))\n",
        "unet = unet.cuda()\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "unet.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "print('model loaded')"
      ],
      "metadata": {
        "id": "qGXoOG2gUf8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading model that was trained on 2 GPUs"
      ],
      "metadata": {
        "id": "_IrJOfVRUX8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unet = Unet(dim=48, channels=1, resnet_block_groups=3, dim_mults=(1, 2, 4, 4))\n",
        "unet = unet.cuda()\n",
        "checkpointm = torch.load('./checkpoint.pth')\n",
        "\n",
        "from collections import OrderedDict\n",
        "checkpoint = OrderedDict()\n",
        "for k, v in checkpointm.items():\n",
        "    if k!='model_state_dict':\n",
        "        checkpoint[k]=v\n",
        "        continue\n",
        "    checkpoint[k]=OrderedDict()\n",
        "    for k1,v1 in v.items():\n",
        "        name = k1[7:] # remove `module.`\n",
        "        checkpoint[k][name] = v1\n",
        "\n",
        "unet.load_state_dict(checkpoint['model_state_dict'])\n",
        "params = list(unet.parameters())\n",
        "optimizer = Adam(params, lr=5e-5)\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "print('model loaded')"
      ],
      "metadata": {
        "id": "Xq_ZMae8UTRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a function that shows a binary 2D array, in our case we will use it to see the generated binary piano rolls."
      ],
      "metadata": {
        "id": "hdUur3zHD3S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def show_image(tens):\n",
        "  tens=np.asarray(dtype=np.dtype('uint8'),a=tens)\n",
        "  temp=((tens))*255\n",
        "  img = Image.fromarray(temp,mode='L').convert('1')\n",
        "  img.save('temp.png')\n",
        "  plt.figure()\n",
        "  plt.imshow(mpimg.imread('temp.png'))"
      ],
      "metadata": {
        "id": "yk3l304zU9zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The get_sample() Function\n",
        "\n",
        "This function is the one that generates samples. It performs 2 tasks:\n",
        "- **Unconditional Generation:** By calling the function without any parameters.\n",
        "- **Infilling:** By calling the function with task='inf', and 'given' is an array that containes the fixed values of the piano roll segment, and -1 in the positions that we want our model to generate.\n",
        "\n",
        "Also this function can generate segments of varying lengths, but this is not recommended."
      ],
      "metadata": {
        "id": "NN-iDHFnELOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample(length=192,task='gen',given=[0]):\n",
        "    total_num_steps = 100\n",
        "    beta = 1.0\n",
        "    ratio = calc_ratio()\n",
        "    noisy_intial = np.random.binomial(1, ratio,size=(length,88))\n",
        "    noisy = np.copy(noisy_intial)\n",
        "    if task=='inf':\n",
        "      for i in range(length):\n",
        "        for j in range(88):\n",
        "          if given[i][j]!=-1:noisy[i][j]=given[i][j]\n",
        "    for i in range(total_num_steps):\n",
        "        noisy_input = noisy.reshape(1, 1, length, 88)  # Add batch and channel dimensions\n",
        "        noisy_tensor = torch.from_numpy(noisy_input.astype(np.float32)).cuda('cuda')\n",
        "        time_tensor = torch.unsqueeze(torch.tensor(total_num_steps - i - 1, dtype=torch.float32).cuda('cuda'),0)\n",
        "\n",
        "        predicted_x0 = unet(noisy_tensor, time_tensor).cpu().detach().numpy()\n",
        "\n",
        "        threshold = 0.5\n",
        "        predicted_x0 = predicted_x0 >= threshold\n",
        "\n",
        "        beta = (total_num_steps-i)/total_num_steps\n",
        "\n",
        "        delta = predicted_x0 ^ noisy_intial\n",
        "        mask = np.random.binomial(1, delta*beta)\n",
        "        noisy = predicted_x0*(1-mask) + noisy_intial * mask\n",
        "        noisy=noisy[0][0]\n",
        "\n",
        "        if task=='inf':\n",
        "          for i in range(length):\n",
        "            for j in range(88):\n",
        "              if given[i][j]!=-1:noisy[i][j]=given[i][j]\n",
        "    return noisy"
      ],
      "metadata": {
        "id": "4iLxz1AbVFGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We noticed that fairly often the model can generate almost empty samples (with almost no notes). By generating samples until the number of notes is above a threshold is an easy but sufficient way to prevent this."
      ],
      "metadata": {
        "id": "S_ey9DQPFnE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(length=192,task='gen',given=[0],thres=0.01):\n",
        "  it=1\n",
        "  while(1):\n",
        "    sample=get_sample(length=length,task=task,given=given)\n",
        "    ratio=calc_sample_ratio(sample)\n",
        "    print(\"iteration:\",it,\"sample ratio:\",ratio)\n",
        "    it=it+1\n",
        "    if ratio>=thres:\n",
        "      return sample"
      ],
      "metadata": {
        "id": "jFN6_VmKVHz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unconditional Generation"
      ],
      "metadata": {
        "id": "gx4YSNPUVhjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample=generate_sample()\n",
        "show_image(sample)"
      ],
      "metadata": {
        "id": "JeBVPGSWVfiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a function that generates a number of *good* samples and saves them in a zip file."
      ],
      "metadata": {
        "id": "FiML97EyF-S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(n=10):\n",
        "  if not os.path.isdir('./samples'):\n",
        "    os.makedirs('./samples')\n",
        "  for i in range(n):\n",
        "    sample=generate_sample()\n",
        "    sample=np.asarray(dtype=np.dtype('uint8'),a=sample)\n",
        "    temp=((sample))*255\n",
        "    img = Image.fromarray(temp,mode='L').convert('1')\n",
        "    img.save('samples/sample'+str(i)+'.png')\n",
        "  !zip -r '/content/samples.zip' './samples'"
      ],
      "metadata": {
        "id": "8BiFfpRYVj6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_samples(n=1)"
      ],
      "metadata": {
        "id": "rn6VYhFmVnAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infilling\n",
        "\n",
        "The ```image.png``` is the content that we want to infill. You can experiment with your files, or files provided in the ```infilling``` folder from the Github directory."
      ],
      "metadata": {
        "id": "Mr1AKg-XVVVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image=Image.open('image.png')\n",
        "image=transform(image)[0].numpy()\n",
        "show_image(image)\n",
        "given=image.copy()\n",
        "\n",
        "#infill middle section of the segment\n",
        "for i in range(48,144):\n",
        "  for j in range(88):\n",
        "    given[i][j]=-1\n",
        "\n",
        "#infill voices from top voice\n",
        "for i in range(192):\n",
        "  for j in range(88):\n",
        "    if given[i][j]==0:\n",
        "      given[i][j]=-1\n",
        "    else:break\n",
        "\n",
        "show_image(given)"
      ],
      "metadata": {
        "id": "M236eUbMVP-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infsample=generate_sample(task='inf',given=given)\n",
        "show_image(infsample)"
      ],
      "metadata": {
        "id": "754r50HaVcE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}