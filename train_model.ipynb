{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "\n",
        "In this notebook we show how to train the diffusion model.\n",
        "\n",
        "Before running the notebook make sure to upload the ```dataset.zip``` file and the ```denoising_diffusion_pytorch.py``` file (can be found [here](https://github.com/Lilac-code/music-diffusion/tree/main)). Also, if there is a ```checkpoint.pth```, upload that as well.\n",
        "\n",
        "If using Kaggle, then upload these as datasets named 'dataset', 'unetfile' and 'checkpoint' respectively."
      ],
      "metadata": {
        "id": "v2iKr6Au_akO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly we unzip the ```dataset.zip``` file (that contains a directory named dataset containing the piano roll segments), and install all the dependencies.\n",
        "\n",
        "If using Kaggle then unziping the file is not neccessary."
      ],
      "metadata": {
        "id": "QFJ8NgpOAJPm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IJBimmbOiFX"
      },
      "outputs": [],
      "source": [
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ema_pytorch\n",
        "!pip install einops\n",
        "!pip install accelerate\n",
        "#!pip install GPUtil #for 2 GPUs"
      ],
      "metadata": {
        "id": "C2Enoln3OpZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from torchvision import transforms as T, utils\n",
        "import torch\n",
        "from torch import nn\n",
        "#from GPUtil import showUtilization as gpu_usage #for 2 GPUs\n",
        "#from torch.nn.parallel import DataParallel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "transform = T.Compose([T.ToTensor()])"
      ],
      "metadata": {
        "id": "EpVOzoTePSxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then prepare the dataset. To do that we transform the images to tensors, and then create a dataloader with those tensors. This dataloader can also add noise on the fly to generate the desired samples."
      ],
      "metadata": {
        "id": "Wy-XXf2xAMNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segments=[]\n",
        "for img in os.listdir('/kaggle/input/dataset/dataset'):   # or just 'dataset'\n",
        "  f = os.path.join('/kaggle/input/dataset/dataset', img)  # or just 'dataset'\n",
        "  image=Image.open(f)\n",
        "  image=transform(image)\n",
        "  segments.append(image)"
      ],
      "metadata": {
        "id": "LZf_wb4AQqFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_ratio():\n",
        "  ratio=0\n",
        "  for im in segments:\n",
        "    ratio+=torch.sum(im).item()\n",
        "  ratio/=len(segments)\n",
        "  ratio/=len(segments[0][0])\n",
        "  ratio/=len(segments[0][0][0])\n",
        "  return ratio"
      ],
      "metadata": {
        "id": "vFaL0UxMQtPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class that on the fly creates a batch from the dataset\n",
        "class Pianoroll(Dataset):\n",
        "    def __init__(self, rolls, ratio):\n",
        "        super(Pianoroll).__init__()\n",
        "        self.rolls = rolls\n",
        "        self.num_steps = 100\n",
        "        self.ratio = ratio\n",
        "    def __len__(self):\n",
        "        return len(self.rolls) * self.num_steps\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        roll = self.rolls[index//self.num_steps]\n",
        "        t = index%self.num_steps\n",
        "        beta = (t+1)/self.num_steps\n",
        "        noisy = np.random.binomial(1, np.asarray(roll)*(1-beta)+self.ratio*beta)\n",
        "        return np.array(noisy,dtype=np.float32), np.array(roll,dtype=np.float32), t\n",
        "\n",
        "# create dataloader_train that feeds the network 50 epochs of the training data with shuffling\n",
        "# segments is the list of piano-roll segment to train the network on\n",
        "# ratio is the ratio of 1s to the size of the piano-roll segment (#rows by #columns)\n",
        "ratio=calc_ratio()\n",
        "pr = Pianoroll(rolls=segments, ratio=ratio)\n",
        "batch_size = 20\n",
        "dataloader_train = DataLoader(pr, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "iLRynVVbRGQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to import the UNet class (and the checkpoint if it exists) in Kaggle we have to copy the file into the working directory."
      ],
      "metadata": {
        "id": "BBt5678MRD3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copyfile\n",
        "\n",
        "# copy file into the working directory (make sure it has .py suffix)\n",
        "copyfile(src = \"/kaggle/input/unetfile/denoising_diffusion_pytorch.py\", dst = \"/kaggle/working/denoising_diffusion_pytorch.py\")\n",
        "\n",
        "# copyfile(src = \"/kaggle/input/checkpoint/checkpoint.pth\", dst = \"/kaggle/working/checkpoint.pth\") #if there is a checkpoint"
      ],
      "metadata": {
        "id": "CQxkBhU_Q0yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making sure that the dataloader has the right lenght - must be equal to the number of segments multiplied by 100 and divided by the batch size."
      ],
      "metadata": {
        "id": "JESUAc7vC1pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('length of dataloader:',len(dataloader_train))"
      ],
      "metadata": {
        "id": "GmhydAsyRY0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for 2 GPUs (Kaggle)"
      ],
      "metadata": {
        "id": "o5Ahh6u7R5NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the unet implementation from https://github.com/lucidrains/denoising-diffusion-pytorch\n",
        "from denoising_diffusion_pytorch import Unet\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "unet = Unet(dim=48, channels=1, resnet_block_groups=3, dim_mults=(1, 2, 4, 4))\n",
        "device_ids = [0, 1]  # Specify the GPU device IDs\n",
        "unet=DataParallel(unet, device_ids=device_ids)\n",
        "unet = unet.cuda(device_ids[0])\n",
        "# Load checkpoint if it exists\n",
        "checkpoint_path = '/kaggle/working/checkpoint.pth'\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    unet.load_state_dict(checkpoint['model_state_dict'])\n",
        "    params = list(unet.parameters())\n",
        "    optimizer = Adam(params, lr=5e-5)\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    params = list(unet.parameters())\n",
        "    optimizer = Adam(params, lr=5e-5)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss_function = nn.L1Loss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    for step,batch in enumerate(dataloader_train):\n",
        "        if step%2==0:\n",
        "            batch1=batch\n",
        "            continue\n",
        "        batch2=batch\n",
        "\n",
        "        batch_roll1 = batch1[0].cuda(device_ids[0])\n",
        "        batch_time1 = batch1[2].cuda(device_ids[0])\n",
        "        batch_roll2 = batch2[0].cuda(device_ids[1])\n",
        "        batch_time2 = batch2[2].cuda(device_ids[1])\n",
        "\n",
        "        predicted_x01 = unet(batch_roll1, batch_time1)\n",
        "        predicted_x02 = unet(batch_roll2, batch_time2)\n",
        "\n",
        "        loss = loss_function(predicted_x01.float(), batch1[1].float().to(device_ids[0]))\n",
        "        loss.backward()\n",
        "        loss = loss_function(predicted_x02.float(), batch2[1].float().to(device_ids[0]))\n",
        "        loss.backward()\n",
        "        if step%10==9:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    # Log the loss at the end of each epoch\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "    # Save a checkpoint at the end of each epoch\n",
        "    checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': unet.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "Bs1VuLsWR1xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for 1 GPU"
      ],
      "metadata": {
        "id": "PgkD2qSzSJ9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "\n",
        "# Load checkpoint if it exists\n",
        "checkpoint_path = './checkpoint.pth'\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    unet.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    unet = Unet(dim=48, channels=1, resnet_block_groups=3, dim_mults=(1, 2, 4, 4))\n",
        "    unet.to('cuda')\n",
        "    params = list(unet.parameters())\n",
        "    optimizer = Adam(params, lr=5e-5)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "loss_function = nn.L1Loss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    for step, batch in enumerate(dataloader_train):\n",
        "        batch_roll = batch[0].cuda()\n",
        "        batch_time = batch[2].cuda()\n",
        "\n",
        "        predicted_x0 = unet(batch_roll, batch_time)\n",
        "\n",
        "        loss = loss_function(predicted_x0.float(), batch[1].float().cuda())\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Log the loss at the end of each epoch\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Save a checkpoint at the end of each epoch\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': unet.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss.item(),\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    if (epoch+1)%5==0:\n",
        "      files.download(checkpoint_path)\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "OgklhZHOR80F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}